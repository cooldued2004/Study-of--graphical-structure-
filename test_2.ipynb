{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd17831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated graph_td_4000.py with 4000 lines.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import heapq\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Tuple, Callable, Iterable, Any\n",
    "\n",
    "# Graph and Temporal Difference utilities + generator to produce a 4000-line code file.\n",
    "# Save this as generate_graph_td.py and run to create a 4000-line file named graph_td_4000.py\n",
    "\n",
    "\n",
    "# ---------- Graph data structures and algorithms ----------\n",
    "\n",
    "class Graph:\n",
    "    \"\"\"Simple directed/undirected weighted graph adjacency list implementation.\"\"\"\n",
    "    def __init__(self, directed: bool = False):\n",
    "        self.adj: Dict[Any, List[Tuple[Any, float]]] = defaultdict(list)\n",
    "        self.directed = directed\n",
    "\n",
    "    def add_edge(self, u, v, weight: float = 1.0):\n",
    "        self.adj[u].append((v, float(weight)))\n",
    "        if not self.directed:\n",
    "            self.adj[v].append((u, float(weight)))\n",
    "\n",
    "    def neighbors(self, u):\n",
    "        return self.adj.get(u, [])\n",
    "\n",
    "    def nodes(self):\n",
    "        return list(self.adj.keys())\n",
    "\n",
    "    def dijkstra(self, source) -> Dict[Any, float]:\n",
    "        \"\"\"Return shortest distances from source to all reachable nodes.\"\"\"\n",
    "        dist = {source: 0.0}\n",
    "        pq = [(0.0, source)]\n",
    "        while pq:\n",
    "            d, u = heapq.heappop(pq)\n",
    "            if d > dist.get(u, math.inf):\n",
    "                continue\n",
    "            for v, w in self.neighbors(u):\n",
    "                nd = d + w\n",
    "                if nd < dist.get(v, math.inf):\n",
    "                    dist[v] = nd\n",
    "                    heapq.heappush(pq, (nd, v))\n",
    "        return dist\n",
    "\n",
    "    def random_walk(self, start, steps: int, rng: random.Random = None):\n",
    "        \"\"\"Yield sequence of nodes visited by a random walk (choose neighbors uniformly).\"\"\"\n",
    "        if rng is None:\n",
    "            rng = random.Random()\n",
    "        u = start\n",
    "        yield u\n",
    "        for _ in range(steps):\n",
    "            nbrs = self.neighbors(u)\n",
    "            if not nbrs:\n",
    "                break\n",
    "            u = rng.choice(nbrs)[0]\n",
    "            yield u\n",
    "\n",
    "    def sample_episode(self, start, policy: Callable[[Any, List[Tuple[Any, float]]], Any], max_steps: int = 100, rng: random.Random = None):\n",
    "        \"\"\"Generate (state, action/state transition, reward) episodes using a policy that picks next node.\"\"\"\n",
    "        if rng is None:\n",
    "            rng = random.Random()\n",
    "        s = start\n",
    "        episode = []\n",
    "        for _ in range(max_steps):\n",
    "            nbrs = self.neighbors(s)\n",
    "            if not nbrs:\n",
    "                break\n",
    "            a = policy(s, nbrs, rng)\n",
    "            # action a is the next state; find weight (optional)\n",
    "            reward = 0.0\n",
    "            # If edges have weights, we might treat weights differently; keep reward separate via callback if needed\n",
    "            episode.append((s, a, reward))\n",
    "            s = a\n",
    "        return episode\n",
    "\n",
    "# ---------- Temporal Difference learning algorithms ----------\n",
    "\n",
    "def td0(graph: Graph, reward_fn: Callable[[Any], float], gamma: float = 0.99,\n",
    "        alpha: float = 0.1, episodes: Iterable[List[Tuple[Any, Any, float]]] = (),\n",
    "        init_value: float = 0.0) -> Dict[Any, float]:\n",
    "    \"\"\"\n",
    "    TD(0) algorithm for state-value estimation.\n",
    "    episodes: iterable of episodes where each episode is a list of (s, s_next, r) tuples.\n",
    "    reward_fn(s_next) used if r omitted.\n",
    "    \"\"\"\n",
    "    V = defaultdict(lambda: init_value)\n",
    "    for ep in episodes:\n",
    "        for (s, s_next, r) in ep:\n",
    "            if r is None:\n",
    "                r = reward_fn(s_next)\n",
    "            td_target = r + gamma * V[s_next]\n",
    "            td_error = td_target - V[s]\n",
    "            V[s] += alpha * td_error\n",
    "    return V\n",
    "\n",
    "def td_lambda(graph: Graph, reward_fn: Callable[[Any], float], gamma: float = 0.99,\n",
    "              alpha: float = 0.1, lam: float = 0.8,\n",
    "              episodes: Iterable[List[Tuple[Any, Any, float]]] = (),\n",
    "              init_value: float = 0.0) -> Dict[Any, float]:\n",
    "    \"\"\"\n",
    "    TD(lambda) with accumulating eligibility traces.\n",
    "    Episodes is an iterable of episodes (s, s_next, r).\n",
    "    \"\"\"\n",
    "    V = defaultdict(lambda: init_value)\n",
    "    for ep in episodes:\n",
    "        E = defaultdict(float)  # eligibility traces\n",
    "        for (s, s_next, r) in ep:\n",
    "            if r is None:\n",
    "                r = reward_fn(s_next)\n",
    "            td_target = r + gamma * V[s_next]\n",
    "            td_error = td_target - V[s]\n",
    "            E[s] += 1.0  # accumulating traces\n",
    "            for state in list(E.keys()):\n",
    "                V[state] += alpha * td_error * E[state]\n",
    "                E[state] *= gamma * lam\n",
    "    return V\n",
    "\n",
    "# ---------- Utility policies and reward functions ----------\n",
    "\n",
    "def uniform_policy(state, neighbors, rng: random.Random = None):\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "    return rng.choice(neighbors)[0]\n",
    "\n",
    "def greedy_policy_factory(value_function: Dict[Any, float], rng_seed: int = None):\n",
    "    rng = random.Random(rng_seed)\n",
    "    def greedy(state, neighbors, rng_local=None):\n",
    "        if rng_local is None:\n",
    "            local = rng\n",
    "        else:\n",
    "            local = rng_local\n",
    "        best = None\n",
    "        best_v = -math.inf\n",
    "        for (nxt, _) in neighbors:\n",
    "            v = value_function.get(nxt, 0.0)\n",
    "            if v > best_v or best is None:\n",
    "                best_v = v\n",
    "                best = nxt\n",
    "        if best is None:\n",
    "            return local.choice(neighbors)[0]\n",
    "        return best\n",
    "    return greedy\n",
    "\n",
    "def example_reward_fn(goal_states: Iterable[Any], reward_for_goal: float = 1.0):\n",
    "    goals = set(goal_states)\n",
    "    def r(s):\n",
    "        return reward_for_goal if s in goals else 0.0\n",
    "    return r\n",
    "\n",
    "# ---------- Convenience: generate episodes on graph ----------\n",
    "\n",
    "def generate_random_walk_episodes(graph: Graph, starts: Iterable[Any], policy: Callable = None,\n",
    "                                  n_episodes: int = 1000, max_steps: int = 50, rng_seed: int = None):\n",
    "    rng = random.Random(rng_seed)\n",
    "    if policy is None:\n",
    "        policy = uniform_policy\n",
    "    episodes = []\n",
    "    for _ in range(n_episodes):\n",
    "        start = rng.choice(list(starts))\n",
    "        ep = []\n",
    "        s = start\n",
    "        for _ in range(max_steps):\n",
    "            nbrs = graph.neighbors(s)\n",
    "            if not nbrs:\n",
    "                break\n",
    "            a = policy(s, nbrs, rng)\n",
    "            # reward left as None -> caller's reward_fn handles it\n",
    "            ep.append((s, a, None))\n",
    "            s = a\n",
    "        episodes.append(ep)\n",
    "    return episodes\n",
    "\n",
    "# ---------- Example experiment combining graph and TD ----------\n",
    "\n",
    "def run_example(seed: int = 0, n_nodes: int = 50, edge_prob: float = 0.08,\n",
    "                gamma: float = 0.95, alpha: float = 0.1, lam: float = 0.8,\n",
    "                n_episodes: int = 2000, max_steps: int = 30):\n",
    "    rng = random.Random(seed)\n",
    "    G = Graph(directed=True)\n",
    "    nodes = list(range(n_nodes))\n",
    "    for u in nodes:\n",
    "        for v in nodes:\n",
    "            if u == v:\n",
    "                continue\n",
    "            if rng.random() < edge_prob:\n",
    "                # weight used only for shortest-path examples; not for transitions\n",
    "                G.add_edge(u, v, weight=rng.random() + 0.01)\n",
    "\n",
    "    # designate a random small set of goal states\n",
    "    goals = set(rng.sample(nodes, max(1, n_nodes // 10)))\n",
    "    reward_fn = example_reward_fn(goals, reward_for_goal=1.0)\n",
    "\n",
    "    episodes = generate_random_walk_episodes(G, starts=nodes, policy=uniform_policy,\n",
    "                                            n_episodes=n_episodes, max_steps=max_steps, rng_seed=seed)\n",
    "\n",
    "    V_td0 = td0(G, reward_fn, gamma=gamma, alpha=alpha, episodes=episodes, init_value=0.0)\n",
    "    V_tdl = td_lambda(G, reward_fn, gamma=gamma, alpha=alpha, lam=lam, episodes=episodes, init_value=0.0)\n",
    "\n",
    "    # compute approximate \"true\" values by solving Bellman equations on a deterministic simplified model:\n",
    "    # We form a linear system V = R + gamma * P * V where P is average-next-state transition.\n",
    "    # Build P and R from empirical transitions\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    total_counts = defaultdict(int)\n",
    "    for ep in episodes:\n",
    "        for (s, s_next, r) in ep:\n",
    "            counts[s][s_next] += 1\n",
    "            total_counts[s] += 1\n",
    "    nodes_list = nodes\n",
    "    idx = {s: i for i, s in enumerate(nodes_list)}\n",
    "    n = len(nodes_list)\n",
    "    # Build transition probabilities P matrix as dict-of-dict\n",
    "    P = {s: {} for s in nodes_list}\n",
    "    R = {s: 0.0 for s in nodes_list}\n",
    "    for s in nodes_list:\n",
    "        if total_counts[s] > 0:\n",
    "            for s2, c in counts[s].items():\n",
    "                P[s][s2] = c / total_counts[s]\n",
    "        else:\n",
    "            # if no outgoing samples, self-loop\n",
    "            P[s][s] = 1.0\n",
    "        # expected immediate reward under P (using reward_fn)\n",
    "        r_exp = 0.0\n",
    "        for s2, p in P[s].items():\n",
    "            r_exp += p * reward_fn(s2)\n",
    "        R[s] = r_exp\n",
    "\n",
    "    # iterative policy evaluation (value iteration on the Markov chain)\n",
    "    V_true = {s: 0.0 for s in nodes_list}\n",
    "    for _ in range(500):\n",
    "        delta = 0.0\n",
    "        newV = {}\n",
    "        for s in nodes_list:\n",
    "            v = R[s] + gamma * sum(p * V_true[s2] for s2, p in P[s].items())\n",
    "            delta = max(delta, abs(v - V_true[s]))\n",
    "            newV[s] = v\n",
    "        V_true = newV\n",
    "        if delta < 1e-6:\n",
    "            break\n",
    "\n",
    "    # compute mean squared error between estimates and V_true on visited states\n",
    "    def mse(Va, Vb):\n",
    "        keys = set(Vb.keys()) | set(Va.keys())\n",
    "        return sum((Va.get(k, 0.0) - Vb.get(k, 0.0))**2 for k in keys) / max(1, len(keys))\n",
    "\n",
    "    stats = {\n",
    "        \"mse_td0\": mse(V_td0, V_true),\n",
    "        \"mse_tdlambda\": mse(V_tdl, V_true),\n",
    "        \"n_nodes\": n_nodes,\n",
    "        \"n_episodes\": n_episodes,\n",
    "        \"goals\": sorted(list(goals))[:10],\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"graph\": G,\n",
    "        \"V_td0\": V_td0,\n",
    "        \"V_tdlambda\": V_tdl,\n",
    "        \"V_true\": V_true,\n",
    "        \"stats\": stats,\n",
    "        \"episodes_sample\": episodes[:5],\n",
    "    }\n",
    "\n",
    "# ---------- Generator to create a large code file with graph + TD content ----------\n",
    "\n",
    "HEADER = \"\"\"# Auto-generated graph + temporal-difference large file\n",
    "# This file was created by generate_graph_td.py to contain detailed implementations,\n",
    "# examples and additional documentation lines to reach a target number of lines.\n",
    "#\n",
    "# It includes:\n",
    "# - Graph data structure and algorithms\n",
    "# - TD(0) and TD(lambda) implementations\n",
    "# - Utilities to run experiments and produce reproducible comparisons\n",
    "#\n",
    "\"\"\"\n",
    "\n",
    "CORE_SNIPPET = '''\n",
    "# -- Core: Graph class (brief) --\n",
    "class GraphMini:\n",
    "    def __init__(self, directed=False):\n",
    "        self.adj = {}\n",
    "        self.directed = directed\n",
    "    def add_edge(self,u,v,w=1.0):\n",
    "        self.adj.setdefault(u,[]).append((v,w))\n",
    "        if not self.directed:\n",
    "            self.adj.setdefault(v,[]).append((u,w))\n",
    "    def neighbors(self,u):\n",
    "        return self.adj.get(u,[])\n",
    "# -- Core: TD(0) demonstration --\n",
    "def demo_td_zero(transitions, gamma=0.9, alpha=0.1, epochs=1):\n",
    "    V = {}\n",
    "    for _ in range(epochs):\n",
    "        for (s,s2,r) in transitions:\n",
    "            V.setdefault(s,0.0)\n",
    "            V.setdefault(s2,0.0)\n",
    "            td = r + gamma*V[s2] - V[s]\n",
    "            V[s] += alpha*td\n",
    "    return V\n",
    "'''\n",
    "\n",
    "def generate_large_file(path: str = \"graph_td_4000.py\", target_lines: int = 4000):\n",
    "    \"\"\"\n",
    "    Generate a Python file containing explanatory header, core snippets and\n",
    "    appended filler comments to reach exactly `target_lines` lines.\n",
    "    \"\"\"\n",
    "    parts = [HEADER, CORE_SNIPPET]\n",
    "    content = \"\\n\".join(parts)\n",
    "    content_lines = content.splitlines()\n",
    "    current_n = len(content_lines)\n",
    "    if current_n >= target_lines:\n",
    "        # If core is already too long (unlikely), truncate\n",
    "        out = \"\\n\".join(content_lines[:target_lines]) + \"\\n\"\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as fh:\n",
    "            fh.write(out)\n",
    "        return path\n",
    "\n",
    "    # Add an example usage block (kept concise)\n",
    "    usage = '''\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: small graph and TD(0) run\n",
    "    g = Graph()\n",
    "    # build a line graph 0->1->2 with terminal 2 as a goal\n",
    "    g.add_edge(0,1)\n",
    "    g.add_edge(1,2)\n",
    "    reward = example_reward_fn({2}, reward_for_goal=1.0)\n",
    "    episodes = [[(0,1,None),(1,2,None)] for _ in range(50)]\n",
    "    V = td0(g, reward, gamma=0.9, alpha=0.1, episodes=episodes)\n",
    "    print(\"V estimates (sample):\", {k: V[k] for k in sorted(list(V))[:10]})\n",
    "'''\n",
    "    full = content + \"\\n\" + usage\n",
    "    lines = full.splitlines()\n",
    "    current_n = len(lines)\n",
    "    # append filler comment lines to reach exactly target_lines\n",
    "    filler_needed = target_lines - current_n\n",
    "    filler = [\"# filler line %d\" % i for i in range(filler_needed)]\n",
    "    final_lines = lines + filler\n",
    "    final_text = \"\\n\".join(final_lines) + \"\\n\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(final_text)\n",
    "    return path\n",
    "\n",
    "# If this module is run, produce the large file\n",
    "if __name__ == \"__main__\":\n",
    "    out = generate_large_file(\"graph_td_4000.py\", target_lines=4000)\n",
    "    print(f\"Generated {out} with 4000 lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22806e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcxcvxcvxcvxcvxcasdsdsadasdasdsdasdsadsaasdasdsgdfgfgdfgdfssadasdasdasdassadsadsdasdasdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e458cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsfsdfsdsdsdfsdfsdfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
